{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1511796b-f76d-47bf-a23b-482554b615f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Linear models\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "# Support Vector Machines\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Tree-based models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Pipelines and preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# External models\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d71cca-027b-42fa-913d-3c0f5f3d11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee74d1f-e955-4345-9d02-160e9d5ba5c7",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7abd4e55-6f8f-4296-9aa3-fcaf2e4ac6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/ml_models_classyfire.pkl\", \"rb\") as f:\n",
    "    models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89f8ee6-1047-4f61-9d8c-c20b9a17fcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': LogisticRegression(solver='liblinear'),\n",
       " 'BernoulliNB': BernoulliNB(),\n",
       " 'Multinomial Naive Bayes': Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                 ('multinomialnb', MultinomialNB())]),\n",
       " 'Linear SVM': LinearSVC(),\n",
       " 'k-NN': KNeighborsClassifier(),\n",
       " 'Random Forest': RandomForestClassifier(),\n",
       " 'LDA': LinearDiscriminantAnalysis(),\n",
       " 'SGD Classifier': SGDClassifier(),\n",
       " 'Lasso Regression': LogisticRegression(penalty='l1', solver='saga'),\n",
       " 'Ridge Regression': RidgeClassifier(),\n",
       " 'Elastic Net': LogisticRegression(l1_ratio=0.5, penalty='elasticnet', solver='saga'),\n",
       " 'Decision Tree': DecisionTreeClassifier(),\n",
       " 'Gradient Boosting': GradientBoostingClassifier()}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd77634-72d7-44fd-994f-c0ed157842a4",
   "metadata": {},
   "source": [
    "# Load data for calibration LinearSVC, SGDClassifier, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0264f7-3189-4b5d-8036-4484b7678648",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/antidiabetic/antidiabetic_classyfire_chemont_taxonomy.pkl\", \"rb\") as f:\n",
    "    antidiabetic_taxonomy_descriptors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d498d871-70f5-4009-8c27-0c77265f6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/non_antidiabetic/non_antidiabetic_classyfire_chemont_taxonomy.pkl\", \"rb\") as f:\n",
    "    non_antidiabetic_taxonomy_descriptors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce10bfb-4d5b-4f6d-8874-e796652cf71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1263/2257884334.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_a = df_a.applymap(lambda x: 1 if isinstance(x, str) else -1)\n",
      "/tmp/ipykernel_1263/2257884334.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_N = df_N.applymap(lambda x: 1 if isinstance(x, str) else -1)\n"
     ]
    }
   ],
   "source": [
    "df_a = pd.DataFrame.from_dict(antidiabetic_taxonomy_descriptors).T\n",
    "df_a = df_a.applymap(lambda x: 1 if isinstance(x, str) else -1)\n",
    "df_N = pd.DataFrame.from_dict(non_antidiabetic_taxonomy_descriptors).T\n",
    "df_N = df_N.applymap(lambda x: 1 if isinstance(x, str) else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2eb8230-a674-4635-97fd-ab8c0e9ff41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a['y'] = 1\n",
    "df_N['y'] = -1\n",
    "\n",
    "df = pd.concat([df_a, df_N], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea150557-b649-4a90-b8a0-acbb824b5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"y\")\n",
    "y = df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1634602-ac4b-4aa5-b86b-605d79abdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13f046-e943-487f-9cee-73c61ad108d9",
   "metadata": {},
   "source": [
    "# Get classyfire chemont taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366c6c94-9b32-4c5d-8c52-51f09e2720f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = '''{\"id\":12133074,\"label\":\"MyQueryLabel\",\"classification_status\":\"Done\",\"number_of_elements\":1,\"number_of_pages\":1,\"invalid_entities\":[],\"entities\":[{\"identifier\":\"Q12133074-1\",\"smiles\":\"CC@HC1=CC=CC=C1\",\"inchikey\":\"InChIKey=WAPNOHKVXSQRPX-ZETCQYMHSA-N\",\"kingdom\":{\"name\":\"Organic compounds\",\"description\":\"Compounds that contain at least one carbon atom, excluding isocyanide/cyanide and their non-hydrocarbyl derivatives, thiophosgene, carbon diselenide, carbon monosulfide, carbon disulfide, carbon subsulfide, carbon monoxide, carbon dioxide, Carbon suboxide, and dicarbon monoxide.\",\"chemont_id\":\"CHEMONTID:0000000\",\"url\":\"http://classyfire.wishartlab.com/tax_nodes/C0000000\"},\"superclass\":{\"name\":\"Benzenoids\",\"description\":\"Aromatic compounds containing one or more benzene rings.\",\"chemont_id\":\"CHEMONTID:0002448\",\"url\":\"http://classyfire.wishartlab.com/tax_nodes/C0002448\"},\"class\":{\"name\":\"Benzene and substituted derivatives\",\"description\":\"Aromatic compounds containing one monocyclic ring system consisting of benzene.\",\"chemont_id\":\"CHEMONTID:0002279\",\"url\":\"http://classyfire.wishartlab.com/tax_nodes/C0002279\"},\"subclass\":null,\"intermediate_nodes\":[],\"direct_parent\":{\"name\":\"Benzene and substituted derivatives\",\"description\":\"Aromatic compounds containing one monocyclic ring system consisting of benzene.\",\"chemont_id\":\"CHEMONTID:0002279\",\"url\":\"http://classyfire.wishartlab.com/tax_nodes/C0002279\"},\"alternative_parents\":[{\"name\":\"Secondary alcohols\",\"description\":\"Compounds containing a secondary alcohol functional group, with the general structure HOC(R)(R') (R,R'=alkyl, aryl).\",\"chemont_id\":\"CHEMONTID:0001661\",\"url\":\"http://classyfire.wishartlab.com/tax_nodes/C0001661\"},{\"name\":\"Hydrocarbon derivatives\",\"description\":\"Derivatives of hydrocarbons obtained by substituting one or more carbon atoms by an heteroatom. They contain at least one carbon atom and heteroatom.\",\"chemont_id\":\"CHEMONTID:0004150\",\"url\":\"http://classyfire.wishartlab.com/tax_nodes/C0004150\"},{\"name\":\"Aromatic alcohols\",\"description\":\"Compounds containing an alcohol group attached to an aromatic carbon.\",\"chemont_id\":\"CHEMONTID:0003073\",\"url\":\"http://classyfire.wishartlab.com/tax_nodes/C0003073\"}],\"molecular_framework\":\"Aromatic homomonocyclic compounds\",\"substituents\":[\"Monocyclic benzene moiety\",\"Secondary alcohol\",\"Organic oxygen compound\",\"Hydrocarbon derivative\",\"Aromatic alcohol\",\"Organooxygen compound\",\"Alcohol\",\"Aromatic homomonocyclic compound\"],\"description\":\"This compound belongs to the class of organic compounds known as benzene and substituted derivatives. These are aromatic compounds containing one monocyclic ring system consisting of benzene.\",\"external_descriptors\":[{\"source\":\"CHEBI\",\"source_id\":\"CHEBI:16346\",\"annotations\":[\"1-phenylethanol\"]}],\"ancestors\":[\"Alcohols and polyols\",\"Aromatic alcohols\",\"Benzene and substituted derivatives\",\"Benzenoids\",\"Chemical entities\",\"Hydrocarbon derivatives\",\"Organic compounds\",\"Organic oxygen compounds\",\"Organooxygen compounds\",\"Secondary alcohols\"],\"predicted_chebi_terms\":[\"secondary alcohol (CHEBI:35681)\",\"organic molecule (CHEBI:72695)\",\"aromatic alcohol (CHEBI:33854)\",\"benzenes (CHEBI:22712)\",\"chemical entity (CHEBI:24431)\",\"oxygen molecular entity (CHEBI:25806)\",\"organic molecular entity (CHEBI:50860)\",\"organooxygen compound (CHEBI:36963)\",\"polyol (CHEBI:26191)\",\"organic hydroxy compound (CHEBI:33822)\",\"alcohol (CHEBI:30879)\",\"benzenoid aromatic compound (CHEBI:33836)\"],\"predicted_lipidmaps_terms\":[],\"classification_version\":\"2.1\"}]}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5a69b2e-7f6a-4cc1-a7e4-eae27d36a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def parse_classyfire_ids(response_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extracts identifiers and names from the response of the ClassyFire API.\n",
    "    \n",
    "    Args:\n",
    "        response_text: A JSON string from the ClassyFire API response\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary where keys are identifiers and values are names\n",
    "        \n",
    "    Raises:\n",
    "        json.JSONDecodeError: if the JSON is malformed\n",
    "        KeyError: if required fields are missing in the data\n",
    "    \"\"\"\n",
    "\n",
    "    result: Dict[str, str] = {}\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(response_text)\n",
    "        \n",
    "        # Get the first entity from the list\n",
    "        if not data.get('entities') or not data['entities'][0]:\n",
    "            return result\n",
    "            \n",
    "        entity = data['entities'][0]\n",
    "        \n",
    "        # Processing predicted_chebi_terms\n",
    "        chebi_terms: List[str] = entity.get('predicted_chebi_terms', [])\n",
    "        for term in chebi_terms:\n",
    "            # Extract CHEBI ID and name from a string in the format \"name (CHEBI:id)\"\n",
    "            if '(' in term and ')' in term:\n",
    "                name, chebi_id = term.split(' (')\n",
    "                chebi_id = chebi_id.rstrip(')')\n",
    "                result[chebi_id] = name\n",
    "        \n",
    "        # Processing all nodes with CHEMONTID\n",
    "        nodes_to_check = [\n",
    "            entity.get('kingdom', {}),\n",
    "            entity.get('superclass', {}),\n",
    "            entity.get('class', {}),\n",
    "            entity.get('subclass', {}),\n",
    "            entity.get('direct_parent', {})\n",
    "        ]\n",
    "        nodes_to_check.extend(entity.get('alternative_parents', []))\n",
    "        nodes_to_check.extend(entity.get('intermediate_nodes', []))\n",
    "        \n",
    "        for node in nodes_to_check:\n",
    "            if node and 'chemont_id' in node and 'name' in node:\n",
    "                result[node['chemont_id']] = node['name']\n",
    "                \n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6859bf-2fea-4379-bd7b-a385cec4694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1263/2237630750.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_taxonomy_descriptors = df_taxonomy_descriptors.applymap(lambda x: 1 if isinstance(x, str) else -1).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHEBI:35681</th>\n",
       "      <th>CHEBI:72695</th>\n",
       "      <th>CHEBI:33854</th>\n",
       "      <th>CHEBI:22712</th>\n",
       "      <th>CHEBI:24431</th>\n",
       "      <th>CHEBI:25806</th>\n",
       "      <th>CHEBI:50860</th>\n",
       "      <th>CHEBI:36963</th>\n",
       "      <th>CHEBI:26191</th>\n",
       "      <th>CHEBI:33822</th>\n",
       "      <th>CHEBI:30879</th>\n",
       "      <th>CHEBI:33836</th>\n",
       "      <th>CHEMONTID:0000000</th>\n",
       "      <th>CHEMONTID:0002448</th>\n",
       "      <th>CHEMONTID:0002279</th>\n",
       "      <th>CHEMONTID:0001661</th>\n",
       "      <th>CHEMONTID:0004150</th>\n",
       "      <th>CHEMONTID:0003073</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CHEBI:35681  CHEBI:72695  CHEBI:33854  CHEBI:22712  CHEBI:24431  \\\n",
       "0            1            1            1            1            1   \n",
       "\n",
       "   CHEBI:25806  CHEBI:50860  CHEBI:36963  CHEBI:26191  CHEBI:33822  \\\n",
       "0            1            1            1            1            1   \n",
       "\n",
       "   CHEBI:30879  CHEBI:33836  CHEMONTID:0000000  CHEMONTID:0002448  \\\n",
       "0            1            1                  1                  1   \n",
       "\n",
       "   CHEMONTID:0002279  CHEMONTID:0001661  CHEMONTID:0004150  CHEMONTID:0003073  \n",
       "0                  1                  1                  1                  1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy_descriptors = parse_classyfire_ids(response_text)\n",
    "#df_taxonomy_descriptors = pd.DataFrame.from_dict(taxonomy_descriptors).T\n",
    "df_taxonomy_descriptors = pd.DataFrame.from_dict(taxonomy_descriptors, orient='index')\n",
    "df_taxonomy_descriptors = df_taxonomy_descriptors.applymap(lambda x: 1 if isinstance(x, str) else -1).T\n",
    "df_taxonomy_descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40a294-7a8e-4b26-b596-1b617300bae6",
   "metadata": {},
   "source": [
    "# Predict antidiabetic effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ea4d20-9983-49c3-912f-e9ef6e15b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing imputers for models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up imputers: 100%|███████████████████| 13/13 [00:00<00:00, 28281.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making predictions with models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   8%|█▉                       | 1/13 [00:00<00:02,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Logistic Regression...\n",
      "    Preparing test data...\n",
      "    Making predictions with Logistic Regression...\n",
      "    ✓ Logistic Regression completed\n",
      "  Processing BernoulliNB...\n",
      "    Preparing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:  23%|█████▊                   | 3/13 [00:00<00:01,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Making predictions with BernoulliNB...\n",
      "    ✓ BernoulliNB completed\n",
      "  Processing Multinomial Naive Bayes...\n",
      "    Preparing test data...\n",
      "    Making predictions with Multinomial Naive Bayes...\n",
      "    ✓ Multinomial Naive Bayes completed\n",
      "  Processing Linear SVM...\n",
      "    Preparing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "Processing models:  31%|███████▋                 | 4/13 [00:00<00:02,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Calibrating Linear SVM...\n",
      "    Making predictions with calibrated Linear SVM...\n",
      "    ✓ Linear SVM completed\n",
      "  Processing k-NN...\n",
      "    Preparing test data...\n",
      "    Making predictions with k-NN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:  46%|███████████▌             | 6/13 [00:01<00:01,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ k-NN completed\n",
      "  Processing Random Forest...\n",
      "    Preparing test data...\n",
      "    Making predictions with Random Forest...\n",
      "    ✓ Random Forest completed\n",
      "  Processing LDA...\n",
      "    Preparing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:  54%|█████████████▍           | 7/13 [00:01<00:01,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Making predictions with LDA...\n",
      "    ✓ LDA completed\n",
      "  Processing SGD Classifier...\n",
      "    Preparing test data...\n",
      "    Calibrating SGD Classifier...\n",
      "    Making predictions with calibrated SGD Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "Processing models:  69%|█████████████████▎       | 9/13 [00:02<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ SGD Classifier completed\n",
      "  Processing Lasso Regression...\n",
      "    Preparing test data...\n",
      "    Making predictions with Lasso Regression...\n",
      "    ✓ Lasso Regression completed\n",
      "  Processing Ridge Regression...\n",
      "    Preparing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "Processing models:  77%|██████████████████▍     | 10/13 [00:02<00:00,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Calibrating Ridge Regression...\n",
      "    Making predictions with calibrated Ridge Regression...\n",
      "    ✓ Ridge Regression completed\n",
      "  Processing Elastic Net...\n",
      "    Preparing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:  85%|████████████████████▎   | 11/13 [00:02<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Making predictions with Elastic Net...\n",
      "    ✓ Elastic Net completed\n",
      "  Processing Decision Tree...\n",
      "    Preparing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|████████████████████████| 13/13 [00:02<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Making predictions with Decision Tree...\n",
      "    ✓ Decision Tree completed\n",
      "  Processing Gradient Boosting...\n",
      "    Preparing test data...\n",
      "    Making predictions with Gradient Boosting...\n",
      "    ✓ Gradient Boosting completed\n",
      "Predictions completed successfully!\n",
      "Models processed: ['Logistic Regression', 'BernoulliNB', 'Multinomial Naive Bayes', 'Linear SVM', 'k-NN', 'Random Forest', 'LDA', 'SGD Classifier', 'Lasso Regression', 'Ridge Regression', 'Elastic Net', 'Decision Tree', 'Gradient Boosting']\n",
      "Predictions shape for each model: [(1,), (1,), (1,), (1,), (1,), (1,), (1,), (1,), (1,), (1,), (1,), (1,), (1,)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_input(df, expected_features, imputer=None, fit_imputer=False):\n",
    "    \"\"\"\n",
    "    Prepare input data by ensuring all expected features are present\n",
    "    and handling missing values.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find missing columns\n",
    "    missing_cols = [col for col in expected_features if col not in df_copy.columns]\n",
    "    \n",
    "    # Add missing columns efficiently using pd.concat\n",
    "    if missing_cols:\n",
    "        missing_df = pd.DataFrame(\n",
    "            np.full((len(df_copy), len(missing_cols)), np.nan),\n",
    "            columns=missing_cols,\n",
    "            index=df_copy.index\n",
    "        )\n",
    "        df_copy = pd.concat([df_copy, missing_df], axis=1)\n",
    "    \n",
    "    # Select only the expected features\n",
    "    df_prepared = df_copy[expected_features]\n",
    "    \n",
    "    # Handle missing values\n",
    "    if imputer is not None:\n",
    "        if fit_imputer:\n",
    "            # Fit and transform (for training data)\n",
    "            df_prepared = imputer.fit_transform(df_prepared)\n",
    "        else:\n",
    "            # Only transform (for test/new data)\n",
    "            df_prepared = imputer.transform(df_prepared)\n",
    "        \n",
    "        # Convert back to DataFrame to maintain column names\n",
    "        df_prepared = pd.DataFrame(df_prepared, columns=expected_features, index=df_copy.index)\n",
    "    \n",
    "    return df_prepared\n",
    "\n",
    "# Store feature names and imputers for each model\n",
    "saved_feature_names = {}\n",
    "saved_imputers = {}\n",
    "\n",
    "print(\"Preparing imputers for models...\")\n",
    "for model_name, model in tqdm(models.items(), desc=\"Setting up imputers\"):\n",
    "    saved_feature_names[model_name] = X_train.columns.tolist()\n",
    "    \n",
    "    # Create and fit imputer for models that can't handle NaN\n",
    "    # Most sklearn models can't handle NaN, only some tree-based and specific models can\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    \n",
    "    # Models that CAN handle NaN natively (very few)\n",
    "    nan_tolerant_models = (HistGradientBoostingClassifier,)\n",
    "    \n",
    "    if isinstance(model, nan_tolerant_models):\n",
    "        saved_imputers[model_name] = None\n",
    "    else:\n",
    "        # All other models need imputation\n",
    "        imputer = SimpleImputer(strategy='median')  # or 'mean', 'most_frequent'\n",
    "        saved_imputers[model_name] = imputer\n",
    "\n",
    "# Make predictions\n",
    "predictions = {}\n",
    "print(\"\\nMaking predictions with models...\")\n",
    "for model_name, model in tqdm(models.items(), desc=\"Processing models\"):\n",
    "    # Get expected features for this model\n",
    "    expected_features = model.feature_names_in_\n",
    "    imputer = saved_imputers[model_name]\n",
    "    \n",
    "    print(f\"  Processing {model_name}...\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    print(f\"    Preparing test data...\")\n",
    "    X_test_prepared = prepare_input(X_test.copy(), expected_features, imputer, fit_imputer=True)\n",
    "    \n",
    "    # Calibrate if needed (only for specific models that don't have predict_proba)\n",
    "    if isinstance(model, (LinearSVC, SGDClassifier, RidgeClassifier)):\n",
    "        print(f\"    Calibrating {model_name}...\")\n",
    "        calibrated = CalibratedClassifierCV(model, cv=\"prefit\")\n",
    "        calibrated.fit(X_test_prepared, y_test)\n",
    "        \n",
    "        print(f\"    Making predictions with calibrated {model_name}...\")\n",
    "        # Prepare input data for prediction\n",
    "        df_input = prepare_input(df_taxonomy_descriptors.copy(), expected_features, imputer, fit_imputer=False)\n",
    "        proba = calibrated.predict_proba(df_input)\n",
    "    else:\n",
    "        print(f\"    Making predictions with {model_name}...\")\n",
    "        # For all other models, prepare input with imputation if needed\n",
    "        df_input = prepare_input(df_taxonomy_descriptors.copy(), expected_features, imputer, fit_imputer=False)\n",
    "        proba = model.predict_proba(df_input)\n",
    "    \n",
    "    predictions[model_name] = proba[:, 1]\n",
    "    print(f\"    ✓ {model_name} completed\")\n",
    "\n",
    "print(\"Predictions completed successfully!\")\n",
    "print(f\"Models processed: {list(predictions.keys())}\")\n",
    "print(f\"Predictions shape for each model: {[pred.shape for pred in predictions.values()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7358441e-a93c-4d6a-8bcd-e1fbe84df489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>BernoulliNB</th>\n",
       "      <th>Multinomial Naive Bayes</th>\n",
       "      <th>Linear SVM</th>\n",
       "      <th>k-NN</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>LDA</th>\n",
       "      <th>SGD Classifier</th>\n",
       "      <th>Lasso Regression</th>\n",
       "      <th>Ridge Regression</th>\n",
       "      <th>Elastic Net</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Gradient Boosting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.994312</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.464445e-214</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.014785</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.009341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Logistic Regression  BernoulliNB  Multinomial Naive Bayes  Linear SVM  \\\n",
       "0             0.001508     0.994312                 0.003167    0.000161   \n",
       "\n",
       "   k-NN  Random Forest            LDA  SGD Classifier  Lasso Regression  \\\n",
       "0   0.6            0.0  9.464445e-214        0.001871          0.014785   \n",
       "\n",
       "   Ridge Regression  Elastic Net  Decision Tree  Gradient Boosting  \n",
       "0          0.000277     0.009341            0.0           0.019485  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba = pd.DataFrame({k: pd.Series(v) for k, v in predictions.items()})\n",
    "predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61400fae-bd86-4c22-b159-533a3269eee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
